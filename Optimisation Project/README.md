The aim of this project is to implement and analyze the steepest descent (gradient descent) method by applying it to two classical benchmark functions in optimization: Rosenbrock’s and Himmelblau’s functions. Through this analysis, the goal is to evaluate the effectiveness of the algorithm in finding local or global minima under various initial conditions and step size strategies. The study seeks to understand how factors such as the function landscape, starting point, and step size influence the algorithm's convergence behavior and computational performance.
